This file implements XGBoost for different test size proportions of a given data set.  The dataset considered for this problem is the California Housing Prices dataset at https://www.kaggle.com/datasets/camnugent/california-housing-prices.  The number test_size corresponds to the percentage of the data set that will be used to evaluate the machine learning model determined by XGBoost applied to the training data.  The remaining percentage (i.e. 1 minus test_size) corresponds to the percentage of the data set that is used to train the machine learning model.

The predicted median_house_value, based on the remaining variables, is compared to the actual median house value.  The inferred model is then evaluated by computing mean squared error (MSE), root mean squared error (RMSE), and R^2.  This program also outputs a scatter plot of actual (median_house_value) values against predicted ones.

Analysis:  We run this program for test_sizes [0.1,0.5,0.9] on the California Housing Prices dataset.  Partial results are: MSE's of [0.217,0.228,0.320], R^2 of [0.837,0.828,0.759].  The relative lack of increase in the MSE suggests that the model's predictions are close to actual predictions on the test data even when 90 percent of the data set is training data, i.e. no overfitting.  Note the bias-variance tradeoff is still present.  More training data means higher variance in the target variable and lower bias.  That the change in bias-variance is low suggests the California Housing Price dataset has strong, clear patterns that are easy to learn from the features.  This seems intuitive.  Most people can guess that all the features, including total_rooms or ocean_proximity, are highly predictive of median_house_price. A less clear relationship between features and target could have a much more meaningful bias-variance tradeoff.

Components of the code:
1. Import the basic libraries including numpy, pandas, matplotlib.pyplot, seaborn, sys, os and set display options.
2. Import the selected portion of libraries from sklearn and xgboost.
3. Obtain the California Housing Price from sklearn.datasets and put them into a DataFrame.
4. Define compute_error_metrics function for true y and predicted y.  Outputs are mse, rmse, and r2.
5. Define train_and_save_predictions function for a given test_size.  Outputs the error metrics to the user and the predictions for each data point in a csv.  Recall test_size is expressed as a percentage of the data in the real interval [0,1].
6. Define print_predictions function given a test_size and the csv from the previous part:  prints the actual median_house_price and predicted median_house_price for each point in the data set.
7. Define plot_results function that plots the results from a file with Actual and Predicted values into a 2D scatter plot.  Takes in the output from the previous part.
8. Driver file that runs 5,6,7 with dependencies on the previous files when given a list of test_size values as an array.  Outputs are the printouts from each step 5,6,7 for each test_size.

